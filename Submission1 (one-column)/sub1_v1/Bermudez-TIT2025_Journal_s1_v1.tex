\documentclass[lettersize,onecolumn]{IEEEtran}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}              % provides \url{...}
%\usepackage{ifthen}          % provides \ifthenelse
\usepackage{cite}             % improves presentation of citations

\usepackage[cmex10]{amsmath}	  % Use the [cmex10] option to ensure compliance
                              % with IEEEXplore (see bare_conf.tex)
\usepackage{amsfonts}
\interdisplaylinepenalty=1000 % As explained in bare_conf.tex
\usepackage{mleftright}       % fix to wrong spacing of \left-,
                 			  % \middle- \right-commands
\usepackage{cgitIEEE}		  % The following package auto scales 
							  % (,{ and [ to be equivalent to \left(,
							  % \left{ and \left[ only in mathmode.
\mleftright




%\usepackage{algorithmic}
%\usepackage{algorithm}
%\usepackage{array}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\usepackage{textcomp}
%\usepackage{stfloats}
%\usepackage{url}
%\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage{orcidlink}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 12/8/2023, 8/9/2021
\usepackage{orcidlink}
\usepackage{balance}



\begin{document}

\title{Title Journal}

\author{Author 1{\orcidlink{0009-0009-2038-9985}},
Author 2{\orcidlink{0000-0001-5597-1718}},
Author 3{\orcidlink{0000-0002-1887-9215}}, and 
Author 4{\orcidlink{0000-0002-2062-131X}}% <-this % stops a space
\thanks{
Author 1 is with the Department of Automatic Control \& Systems Engineering, The University of Sheffield, Sheffield S1 3JD, U.K.; and also with INRIA, Centre Inria d'Universit\'e C\^ote d'Azur, 06902 Sophia Antipolis, France (e-mail: jdaunastorres1@sheffield.ac.uk).

Author 2 is with the Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield S1 3JD, U.K.; and also with the Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544 USA (e-mail: esnaola@sheffield.ac.uk).

Author 3 is with INRIA, Centre Inria d'Universit\'e C\^ote d'Azur, 06902 Sophia Antipolis, France; also with the Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544 USA; and also with the GAATI Mathematics Laboratory, University of French Polynesia, 98702 Faaa, French Polynesia (e-mail: samir.perlaza@inria.fr).

Author 4 is with the Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544 USA (e-mail: poor@princeton.edu).

This paper was presented in part at the IEEE International Symposium on Information Theory (ISIT), Taipei, Taiwan, Jun., 2023 \cite{Perlaza-ISIT2023a}; and appears as an INRIA Technical Report in \cite{InriaRR9508}.
}
}


\maketitle

\begin{abstract}
Abstract of the Journal
\end{abstract}

\begin{IEEEkeywords}
Supervised Learning, Empirical Risk Minimization; Relative Entropy; Regularization; Gibbs Measure; Inductive Bias; Gibbs Algorithm; Sensitivity; and Generalization.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{E}{mpirical} risk minimization (ERM) is a central tool in supervised machine learning. Among other uses, it enables the characterization of sample complexity and probably approximately correct (PAC) learning in a wide range of settings~\cite{vapnik1992principles}.
%
The application of ERM in the study of theoretical guarantees spans related disciplines such as machine learning \cite{vapnik1964perceptron}, information theory \cite{rodrigues2021information,mezard2009information} and statistics \cite{wainwright2019high, vershynin2018high}.
%
Classical problems such as classification\cite{blumer1989learnability,guyon1991structural}, pattern recognition \cite{lugosi1995nonparametric,bartlett1998sample}, regression \cite{vapnik1993local, cherkassky1999model}, and density estimation \cite{lugosi1995nonparametric,vapnik1999overview} can be posed as special cases of the ERM problem \cite{vapnik1999overview,bottou2018optimization}.
%
Unfortunately, ERM is prone to training data memorization, a phenomenon also known as overfitting \cite{krzyzak1996nonparametric,deng2009regularized,arpit2017Memorization}. For that reason, ERM is often regularized in order to provide generalization guarantees \cite{hellstrom2023generalization, bousquet2002stability,vapnik2015uniform,aminian2021exact}.
%
Regularization establishes a preference over the models by encoding features of interest that conform to prior knowledge.
%
In different statistical learning frameworks, such as Bayesian learning \cite{robert2007bayesian,mcallester1998pacBayesian} and PAC learning \cite{valiant1984theory,shawe1997pac,cullina2018pac}, the prior knowledge over the set of models can be described by a reference probability measure.
%
More general references can be adapted as proved in \cite{Perlaza-ISIT-2022, perlaza2024ERMRER} for the case of $\sigma$-finite measures. Prior knowledge of the set of datasets can also be represented by probability measures, e.g., the worst-case data-generating probability measure introduced in \cite{zou2024WorstCase}.
%
In either case, the solution to the regularized ERM problem can be cast as a probability distribution over the set of models.

A common regularizer of the ERM problem is the relative entropy of the optimization probability measure with respect to a given reference measure over the set of models \cite{vapnik1999overview,raginsky2016information,russo2019much,zou2009generalization}.
%
The resulting problem formulation, termed ERM with relative entropy regularization (ERM-RER) has been extensively studied for both the case in which the reference measure is a probability measure \cite{raginsky2016information,russo2019much,zou2009generalization, aminian2022information} and the case in which it is a $\sigma$-finite measure \cite{Perlaza-ISIT-2022, perlaza2024ERMRER, Perlaza-ISIT2023b}. While in both cases the solution is unique and corresponds to a Gibbs probability measure, the existence of the solution is ensured only in the case in which the reference measure is a probability measure~\cite{perlaza2024ERMRER}. 
%
Despite the many merits of the ERM-RER formulation, it has some significant limitations.
%
Firstly, the absolute continuity of the optimization measure with respect to the reference measure is required for the existence of the corresponding Radon-Nikodym derivative, which is used by the relative entropy regularization. This absolute continuity sets an insurmountable barrier to the exploration of models outside the support of the reference measure. More specifically, models outside the support of the reference measure exhibit zero probability with respect to the Gibbs probability measure solution to ERM-RER, regardless of the evidence provided by the training dataset.
%
Secondly, the choice of relative entropy over alternative divergences often follows arguments based on the simplicity of obtaining generalization guarantees in the form of bounds \cite{hellstrom2023generalization}. Nonetheless, such bounds are often hard to calculate and are not always informative when evaluated in practical settings \cite{Perlaza-ISIT2023a, wang2004enhancing,lin2014accelerated,yang2022estimation, zou2024WorstCase, perlaza2024ERMRER, Wu2024OnGeneralization, borjas2024thesis}.

In view of these, exploring the asymmetry of relative entropy is of particular interest to advancing the understanding of entropy regularization in the context of ERM and its role in generalization. Additionally, examining the asymmetry opens novel pathways to overcome some of the constraints imposed by relative entropy regularization. \IEEEpubidadjcol
% Add transport paper 
The problem of ERM with a general $f$-divergence regularization has been explored in \cite{teboulle1992entropic} and \cite{beck2003mirror} in the case of a finite countable set of models, and recently extended to uncountable sets of models in \cite{alquier2021non} and \cite{esposito2022GenfDiv}. The authors in \cite{teboulle1992entropic, beck2003mirror, alquier2021non, esposito2022GenfDiv} constrain the optimization domains to sets of measures that are mutually absolutely continuous with respect to the reference probability measure. 
%
The use of the relative entropy of the optimization measure with respect to the reference measure as a regularizer in the ERM-RER is termed \mbox{Type-I} ERM-RER. Alternatively, the use of the relative entropy of the reference measure with respect to the optimization measure is termed \mbox{Type-II} ERM-RER.
%
Interestingly, the existing results in \cite{teboulle1992entropic, beck2003mirror, alquier2021non}, which lead to special cases of the \mbox{Type-I} and \mbox{Type-II} ERM-RER problems by assuming that $f(x) = -x \log(x)$ and $f(x) = -\log(x)$,  respectively, do not study the impact of the asymmetry of relative entropy.
%
Another observation that motivates studying the asymmetry of relative entropy in ERM-RER is that numerical analyses of the \mbox{Type-II} ERM-RER, presented in Section~\ref{sec:logERM_RER}, suggest that it achieves better generalization capabilities compared to \mbox{Type-I} ERM-RER, while maintaining a similar expected empirical risk.

This paper presents the solution to \mbox{Type-II} ERM-RER optimization problem using a new method of proof. In particular,  mutual absolute continuity between the measures involved is not imposed. Nonetheless, mutual absolute continuity is exhibited by the solution as a consequence of the structure of the problem. The key properties of the solution are highlighted, and an equivalence between the \mbox{Type-I} and \mbox{Type-II} ERM-RER problems is presented. This equivalence is achieved by replacing the empirical risk in the \mbox{Type-I} ERM-RER problem with another function, which can be interpreted as a tunable loss function, as described in \cite{liao2018tunable, sypherd2019tunable, kurri2021realizing}.
%
The remainder of the paper is organized as follows. Section~\ref{sec:ERMproblem} presents the ERM-RER problem and its  two variations: \mbox{Type-I} and \mbox{Type-II}. 
%
%Section~\ref{sec:Type1ERM_RER} describes the \mbox{Type-I} regularization.
%
The main contribution of this paper, which is the solution to the \mbox{Type-II} ERM-RER problem, is presented in Section~\ref{sec:SolutionType2}. This section also presents key properties of the solution. Section~\ref{sec:ExpectedER} uses these properties to characterize the expected empirical risk.
%
Section~\ref{sec:logERM_RER} studies the equivalence between \mbox{Type-I}  and \mbox{Type-II} ERM-RER problems.
%
This work is concluded by Section~\ref{sec:FinalRemarks}, with some final remarks.



The mathematical notation used throughout the paper is as follows:
Given a measurable space $\msblspc{\set{M}}$, $\bigtriangleup(\set{M})$ is used to represent the set of probability measures that can be defined over $\msblspc{\set{M}}$. Often, when the sigma-algebra $\field{F}$ is fixed, it is hidden to ease notation. Given a probability measure $Q \in \bigtriangleup(\set{M})$, the subset $\bigtriangleup_{Q}(\set{M})$ of $\bigtriangleup(\set{M})$ contains all probability measures that are absolutely continuous with respect to the measure $Q$. Similarly, the subset $\bigtriangledown_{Q}(\set{M})$ of $\bigtriangleup(\set{M})$ contains all probability measures $P \in \bigtriangleup(\set{M})$ such that the probability measure $Q$ is absolutely continuous with respect to $P$. Finally, the subset $\bigcirc_{Q}(\set{M})$ of $\bigtriangleup(\set{M})$ contains all probability measures that are mutually absolutely continuous with respect to the measure $Q$.

%-------------------------------------------------%
%
%           Empirical Risk Minimization
%
%-------------------------------------------------%
\section{Empirical Risk Minimization}
\label{sec:ERMproblem}
%
Let~$\set{M}$,~$\set{X}$ and~$\set{Y}$, with~$\set{M} \subseteq \reals^{d}$ and~$d \in \ints$, be sets of \emph{models}, \emph{patterns}, and \emph{labels}, respectively.  
%
A pair~$(x,y) \in \mathcal{X} \times \mathcal{Y}$ is referred to as a \emph{labeled pattern} or as a \emph{data point}.
%
Given~$n$ data points, with~$n \in \ints$,  denoted by~$\left(x_1, y_1 \right)$, $\left( x_2, y_2\right)$, $\ldots$, $\left( x_n, y_n \right)$, the corresponding dataset is represented by the tuple
\begin{equation}\label{EqTheDataSet}
\vect{z} = \big(\left(x_1, y_1 \right), \left(x_2, y_2 \right), \ldots, \left(x_n, y_n \right)\big)  \in \left( \set{X} \times \set{Y} \right)^n.
\end{equation}  

Let the function~$f: \set{M} \times \mathcal{X} \rightarrow \mathcal{Y}$ be such that the label assigned to the pattern $x$ according to the model $\vect{\theta} \in \set{M}$ is $f(\vect{\theta}, x)$.
%
Let also the function 
\begin{equation}\label{EqEll}
\ell: \set{Y} \times \set{Y} \rightarrow [0, \infty)
\end{equation} 
be such that given a data point~$(x, y) \in \set{X} \times \set{Y}$, the  risk induced by a model~$\vect{\theta} \in \set{M}$ is ~$\ell\left( f(\vect{\theta}, x), y \right)$.  
%
In the following, the risk function~$\ell$ is assumed to be nonnegative and  for all~$y \in \set{Y}$, ~$\ell\left( y , y\right) = 0$.

The \emph{empirical risk} induced by the model~$\vect{\theta}$, with respect to the dataset $\vect{z}$ in~\eqref{EqTheDataSet} is determined by the  function~$\mathsf{L}_{\vect{z}}: \set{M} \rightarrow [0, \infty )$, which satisfies  
\begin{IEEEeqnarray}{rCl}
\label{EqLxy}
\mathsf{L}_{\vect{z}} \left(\vect{\theta} \right)  & = & 
\frac{1}{n}\sum_{i=1}^{n}  \ell\left( f(\vect{\theta}, x_i), y_i\right).
\end{IEEEeqnarray}
%
Using this notation, the ERM consists of the following optimization problem:
%
\begin{equation}\label{EqOriginalOP}
\min_{\vect{\theta} \in \set{M}} \mathsf{L}_{\vect{z}} \left(\vect{\theta} \right).
\end{equation}
%
Let the set of solutions to the ERM problem in~\eqref{EqOriginalOP} be denoted by
\begin{equation}\label{EqHatTheta}
\set{T}\left( \vect{z} \right) \triangleq \arg\min_{\vect{\theta} \in \set{M}}    \mathsf{L}_{\vect{z}} \left(\vect{\theta} \right).
\end{equation}
%
Note that if the set $\set{M}$ is finite, the ERM problem in~\eqref{EqOriginalOP} always possesses a solution, and thus, $\abs{\set{T}\left( \vect{z} \right)} > 0$. Nonetheless, in general, the ERM problem does not necessarily possess a solution, \ie, it might happen that $\abs{\set{T}\left( \vect{z} \right) } = 0$.

%-------------------------------------------------%
%            Statistical Learning
%-------------------------------------------------%
%\subsection{Statistical Learning}
%
The PAC and Bayesian frameworks, as discussed in \cite{mcallester1998pacBayesian} and \cite{shawe1997pac},  address the problem in \eqref{EqOriginalOP} by constructing probability measures, conditioned on the dataset $\dset{z}$, from which models are randomly sampled.
%
In this context, finding probability measures that are minimizers of the ERM problem in~\eqref{EqOriginalOP} over the set of all probability measures that can be defined on the measurable space~$\msblspc{\set{M}}$, which is denoted by $\bigtriangleup(\set{M})$, requires a metric that enables assessing the goodness of the probability measure.
%
From this perspective, the underlying assumption in the remainder of this work is that the functions $f$ and $\ell$  in \eqref{EqLxy} are such that for all $(x,y) \in \set{X} \times \set{Y}$,  the function $g_{x,y}:\set{M}\rightarrow[0, \infty)$, such that $g_{x,y}(\thetav)=\ell(f(\thetav,x),y)$, is measurable with respect to the Borel measurable spaces $(\set{M}, \field{F})$ and $\left( \reals, \BorSigma{\reals} \right)$, where $\field{F}$ and $\BorSigma{\reals}$ are respectively the Borel $\sigma$-fields on $\set{M}$ and $\reals$. 
%-------------------------------------------------%
%            Definition dataset
%-------------------------------------------------%
Under these assumptions, a common metric is the notion of expected empirical risk.
%
\begin{definition}[Expected Empirical Risk]
\label{DefEmpiricalRisk}
Given the dataset \mbox{$\dset{z} \in  ( \set{X} \times \set{Y} )^n$} in~\eqref{EqTheDataSet},  let  the functional $\mathsf{R}_{\dset{z}}: \bigtriangleup(\set{M}) \rightarrow  [0, \infty  )$~be such that
%
\begin{equation}
\label{EqRxy}
\foo{R}_{\dset{z}}( P ) \triangleq \int \foo{L}_{ \dset{z} } ( \thetav )  \diff P(\thetav),
\end{equation}
%
where the function $\foo{L}_{\dset{z}}$ is defined in~\eqref{EqLxy}.
%
\end{definition}
%-------------------------------------------------%

In the following section, the \mbox{Type-I} relative entropy regularization is reviewed as it serves as the basis for the analysis of the regularization asymmetry.


%-------------------------------------------------%
%
%            The \mbox{Type-I} ERM-RER Problem
%
%-------------------------------------------------%
\subsection{The \mbox{Type-I} ERM-RER Problem}
\label{sec:Type1ERM_RER}
%
The \mbox{Type-I} ERM-RER problem is parametrized by a probability measure $Q \in \bigtriangleup(\set{M})$ and a real $\lambda \in \left( 0, \infty\right)$. The measure $Q$ is referred to as the \emph{reference measure} and $\lambda$ as the  \emph{regularization factor}. 
%
The {Type-I} ERM-RER problem, with parameters~$Q$ and~$\lambda$, is given by the following optimization problem:
%
%\begin{subequations}
\begin{IEEEeqnarray}{rCl}
\label{EqOpType1ERMRERNormal}
    \min_{P \in \bigtriangleup_{Q}(\set{M}) } &\ & \foo{R}_{\dset{z}} ( P )  + \lambda \KL{P}{Q},%\\
    %\textrm{s.t.} \qquad \  &\ &  \int \frac{\diff P}{\diff Q}(\thetav) \, \diff Q(\thetav) = 1,
%\label{EqConstratinTrype1}
\end{IEEEeqnarray}
%\end{subequations}
%
where the functional~$\foo{R}_{\dset{z}}$ is defined in~\eqref{EqRxy}, and the optimization domain~is 
%
\begin{IEEEeqnarray}{rCl}
\label{DefSetTriangUp}
		\bigtriangleup_{Q}(\set{M})   & \triangleq & \{P\in \bigtriangleup(\set{M}): P\ll Q \},
\end{IEEEeqnarray}
%
with the notation~$P\ll Q$ standing for~$P$ being absolutely continuous with respect to~$Q$.


%-------------------------------------------------%
%     The Solution \mbox{Type-I} ERM-RER Problem
%-------------------------------------------------%
%
The solution to the \mbox{Type-I} ERM-RER problem in~\eqref{EqOpType1ERMRERNormal} is the Gibbs probability measure reported in \cite{Perlaza-ISIT-2022, raginsky2016information} and \cite{russo2019much}. In order to introduce such a measure, consider the function $K_{Q, \dset{z}}: (0,\infty) \rightarrow \reals$ that satisfies for all $t \in \reals$,
%
\begin{equation}
\label{EqDefKfunction}
		K_{Q, \dset{z}}( t ) = \log (\int \exp(t\foo{L}_{\dset{z}}(\thetav))\diff Q(\thetav)),
\end{equation}
%
with $\foo{L}_{\dset{z}}$ in \eqref{EqLxy}.
%
Using this notation, the solution to the  \mbox{Type-I} ERM-RER problem in~\eqref{EqOpType1ERMRERNormal} is  presented by the following lemma.

%-------------------------------------------------%
%     Theorem Type-I ERM-RER
%-------------------------------------------------%
%
\begin{lemma}[\text{\cite[Theorem $3$]{perlaza2024ERMRER}}]
\label{lemm_OptimalModelType1}
%
The solution to the optimization problem in \eqref{EqOpType1ERMRERNormal} is a unique probability measure, denoted by $\Pgibbs{P}{Q}$, which satisfies for all $\thetav \in \supp Q$,
%The solution of the \mbox{Type-I} ERM-RER problem in~\eqref{EqOpType1ERMRERNormal}, denoted by $\Pgibbs{P}{Q}\!\in\! \bigtriangleup(\set{M})$, is unique, always exists, and satisfies for all~$\thetav \in \supp Q$
%
\begin{equation}
\label{EqGenpdfType1}
\frac{\diff \Pgibbs{P}{Q}}{\diff Q} ( \thetav ) = \exp(-K_{Q, \dset{z}}(-\frac{1}{\lambda})-\frac{1}{\lambda}\foo{L}_{\dset{z}}(\thetav)),
\end{equation}
%
where the function $\foo{L}_{\dset{z}}$ is defined in~\eqref{EqLxy} and the function $K_{Q, \dset{z}}$ is defined in~\eqref{EqDefKfunction}.
%%
%\begin{equation}\label{EqDefKfunction}
%		K_{Q, \dset{z}}( t ) = \log (\int \exp(t\foo{L}_{\dset{z}}(\thetav))\diff Q(\thetav)).
%\end{equation}
\end{lemma} 
%-------------------------------------------------%


%-------------------------------------------------%
%
%       \mbox{Type-II} ERM-RER
%
%-------------------------------------------------%
\subsection{The \mbox{Type-II} ERM-RER Problem}
\label{sec:Type2ERM_RER}

The \mbox{Type-II} ERM-RER problem is parametrized by a probability measure $Q \in \bigtriangleup(\set{M})$ and a real $\lambda \in (0, \infty)$.
%
As in the \mbox{Type-I} ERM-RER problem, the measure $Q$ is referred to as the \emph{reference measure} and $\lambda$ as the \emph{regularization factor}.
%
Given the dataset~$\dset{z} \in (\set{X} \times \set{Y})^n$ in~\eqref{EqTheDataSet}, the \mbox{Type-II} ERM-RER problem, with parameters~$Q$ and~$\lambda$, consists of the following optimization problem:
%
\begin{IEEEeqnarray}{rcl}
\label{EqOpType2ERMRERNormal}
    \min_{P \in \bigtriangledown_{Q}(\set{M}) } &\ & \foo{R}_{\dset{z}} ( P )  + \lambda \KL{Q}{P},
\end{IEEEeqnarray}
%
where the functional~$\foo{R}_{\dset{z}}$ is defined in~\eqref{EqRxy}, and the optimization domain~is 
%
\begin{IEEEeqnarray}{rCl}
\label{DefSetTriangDown}
		\bigtriangledown_{Q}(\set{M}) & \triangleq & \{P\in \bigtriangleup(\set{M}): Q\ll P \}.
\end{IEEEeqnarray}
%
The difference between \mbox{Type-I} and \mbox{Type-II} ERM-RER problems lies on the regularization. While the former uses the relative entropy $\KL{P}{Q}$, the latter uses $\KL{Q}{P}$. 
%
This translates into different optimization domains due to the asymmetry of the relative entropy.
%
More specifically, in the \mbox{Type-I} ERM-RER problem, the optimization domain is the set of probability measures on the Borel measurable space $\msblspc{\set{M}}$ that are absolutely continuous with the reference measure $Q$. That is, the set $\bigtriangleup_{Q}(\set{M})$ in~\eqref{DefSetTriangUp}.
%
Alternatively, in the \mbox{Type-II} ERM-RER problem, the optimization domain consists of probability measures defined on the Borel measurable space $\msblspc{\set{M}}$, with the additional condition that the reference measure $Q$ must be absolutely continuous with respect to them. This corresponds to the set denoted as $\bigtriangledown_{Q}(\set{M})$ in~\eqref{DefSetTriangDown}.
%
From this perspective, the techniques used in \cite{perlaza2024ERMRER} for solving the \mbox{Type-I} ERM-RER no longer hold. As shown in the next section, a new technique is used for solving the \mbox{Type-II} ERM-RER.

The problems in \eqref{EqOpType1ERMRERNormal} and \eqref{EqOpType2ERMRERNormal} exhibit a trivial solutions when the functional $\foo{R}_{\dset{z}}$ is such that for all $P \in \bigtriangleup_{Q}(\set{M})$ or $P \in \bigtriangledown_{Q}(\set{M})$, respectively, it holds that $\foo{R}_{\dset{z}}(P) = c$, for some $c \in [0, \infty)$. In such a case, the solution is unique and equal to the probability measure $Q$, independently of the parameter $\lambda$. In order to avoid this trivial case, the notion of separability of the empirical risk function with respect to the measure $Q$ is borrowed from \cite{perlaza2024ERMRER}. A separable empirical risk function with respect to a given probability measure $P $ is defined as follows.

\begin{definition}[Definition 5 in \cite{perlaza2024ERMRER}]\label{DefSeparableLxy}
The empirical risk function~$\mathsf{L}_{\vect{z}}$ in~\eqref{EqLxy} is said to be separable with respect to the probability measure~$P \in \bigtriangleup(\set{M})$, if there exist a positive real~$c > 0$  and two subsets~$\set{A}$ and~$\set{B}$ of~$\set{M}$  that are nonnegligible with respect to~$P$, and for all~$(\vect{\theta}_1, \vect{\theta}_2) \in \set{A} \times \set{B}$, 
\begin{IEEEeqnarray}{rcl}
\label{EqTwoNonnegligibleSets}
 \mathsf{L}_{\vect{z}} \left( \vect{\theta}_1 \right) &< c <& \mathsf{L}_{\vect{z}}\left(\vect{\theta}_2\right) < \infty .
\end{IEEEeqnarray}
\end{definition}
%
A nonseparable empirical risk function~$\mathsf{L}_{\vect{z}}$ in~\eqref{EqLxy} with respect to a measure $P$ is a constant almost surely with respect to the measure~$P$. More specifically, there exists a real~$a \geq 0$, such that
\begin{equation}
P\left( \left\lbrace \vect{\theta} \in \set{M}: \mathsf{L}_{\vect{z}}\left(\vect{\theta}\right)  = a \right\rbrace\right) = 1.
\end{equation}
%
When the empirical risk function~$\mathsf{L}_{\vect{z}}$ in~\eqref{EqLxy} is nonseparable with respect to all measures in $P \in \bigtriangledown_{Q}(\set{M})$, the trivial case described above is observed. The notion of separable empirical risk functions would play a central role in the study of the optimization problem in \eqref{EqOpType2ERMRERNormal}. 

%-------------------------------------------------%
%
%  The Solution \mbox{Type-II} to the ERM-RER
%
%-------------------------------------------------%
\section{The Solution to the \mbox{Type-II} ERM-RER Problem}
\label{sec:SolutionType2}
 
The solution of the \mbox{Type-II} ERM-RER problem in~\eqref{EqOpType2ERMRERNormal} is presented in the following theorem.
% under the following assumption:
% \begin{equation}\label{EqZeroInfinity}
% Q\left( \left\lbrace \vect{\theta} \in \set{M}:  \mathsf{L}_{\vect{z}}\left( \vect{\theta}\right) = \infty \right\rbrace  \right) =0,
% \end{equation}  
%where the measure $Q$ is a parameter of the optimization problem in~\eqref{EqOpType2ERMRERNormal} and the function $\mathsf{L}_{\vect{z}}$ is defined in~\eqref{EqLxy}.
%-------------------------------------------------%
%        Theorem Radon-Nikodym Derivative
%-------------------------------------------------%
\begin{theorem}
\label{Theo_ERMType2RadNikMutualAbs}
If there exists a real $\beta$ such that
%
\begin{subequations}
\label{EqType2KrescConstrainAll}
\begin{equation}
\label{EqType2KrescConstrain1}
	\beta \in \{t\in \reals: \forall \thetav \in \supp Q, 0 < {t + \foo{L}_{\dset{z}}(\thetav)}\},
\end{equation}
%
and
%
\begin{equation}
\label{EqType2KrescConstrain2}
	\int \frac{\lambda}{\beta + \foo{L}_{\dset{z}}(\thetav)} \diff Q(\thetav) = 1,
\end{equation}
\end{subequations}
%
with the function $\foo{L}_{\dset{z}}$ defined in \eqref{EqLxy}, and $\lambda$ and $Q$ the parameters of the optimization problem in~\eqref{EqOpType2ERMRERNormal},
%If \eqref{EqZeroInfinity} holds, then 
then, the solution to such a problem, denoted by $\Pgibbs{\bar{P}}{Q}\!\in\! \bigtriangleup(\set{M})$, is unique and for all $\thetav \in \supp Q$, it satisfies  
%
\begin{equation}
\label{EqGenpdfType2}
\frac{\diff \Pgibbs{\bar{P}}{Q}}{\diff Q} ( \thetav ) =  \frac{\lambda}{\beta + \foo{L}_{\dset{z}}(\thetav)}.
\end{equation}
%
\end{theorem}
%-------------------------------------------------%

Before introducing the proof of Theorem~\ref{Theo_ERMType2RadNikMutualAbs}, two important results are presented. The first result consists in the solution to the optimization problem in~\eqref{EqOpType2ERMRERNormal}  when the optimization domain is restricted to 
\begin{equation}
	\label{Eq_ProofThT2_setofmeasures}
		\bigcirc_Q(\set{M}) \triangleq \bigtriangledown_{Q}(\set{M}) \cap \bigtriangleup_{Q}(\set{M}),
	\end{equation}
	%
	where the sets~$\bigtriangleup_{Q}(\set{M})$ and~$\bigtriangledown_{Q}(\set{M})$ are defined in~\eqref{DefSetTriangUp} and~\eqref{DefSetTriangDown}, respectively.
	%
Such an ancillary problem can be formulated as follows:
\begin{IEEEeqnarray}{rcl}
	\label{EqOpType2ERMRERancillary}
	\min_{P \in \bigcirc_{Q}(\set{M}) } &\ & \foo{R}_{\dset{z}} ( P )  + \lambda \KL{Q}{P}.
	\end{IEEEeqnarray}
%
The solution to the problem in \eqref{EqOpType2ERMRERancillary} is described by the following lemma. 
	%-------------------------------------------------%
	%       Lemma Radon-Nikodym Derivative Mutual
	%-------------------------------------------------%
	\begin{lemma}
	\label{lemm_Type2RNDbigcirc}
The solution to the optimization problem in \eqref{EqOpType2ERMRERancillary} 
is unique and identical to the probability measure~$\Pgibbs{\bar{P}}{Q}$ in~\eqref{EqGenpdfType2}.
	\end{lemma}
	\begin{IEEEproof}
The proof is presented in Appendix~\ref{app_lemm_Type2RNDbigcirc}.
	\end{IEEEproof}

The second result consists of comparing the optimal values resulting from the optimization problems in  \eqref{EqOpType2ERMRERNormal} and \eqref{EqOpType2ERMRERancillary}, as shown hereunder.

	%-------------------------------------------------%
	%        Lemma present \mbox{Type-II} as exp()
	%-------------------------------------------------%
	\begin{lemma}
	\label{lemm_RadNikDevMutualIneq}
	The optimization problems in \eqref{EqOpType2ERMRERNormal}   and \eqref{EqOpType2ERMRERancillary} satisfy
%
	\begin{equation}
	\label{Eq_ProofT2IneOp2}
		\min_{P \in \bigtriangledown_{Q}} \foo{R}_{\dset{z}}(P) + \lambda \KL{Q}{P}
		\geq \min_{P \in \bigcirc_{Q}} \foo{R}_{\dset{z}}(P) + \lambda \KL{Q}{P}.
	\end{equation}
	\end{lemma}
	%
	\begin{IEEEproof}
		The proof is presented in Appendix~\ref{AppProof_lemm_RadNikDevMutualIneq}.
		%The proof is presented in \cite{InriaRR9508}.
	\end{IEEEproof}
	%-------------------------------------------------%
Lemma~\ref{lemm_RadNikDevMutualIneq} unveils the fact that the objective function in \eqref{EqOpType2ERMRERNormal} when evaluated at measures whose support extends beyond the support of $Q$ is larger than such an objective function evaluated at measures whose support is identical to the reference measure.
%
This includes the case in which the set $\set{T}(\dset{z})$ in~\eqref{EqHatTheta} lies outside the support of $Q$. 
%
Using these results, the proof of  Theorem~\ref{Theo_ERMType2RadNikMutualAbs} is as follows.

\begin{IEEEproof}[Proof of  Theorem~\ref{Theo_ERMType2RadNikMutualAbs}]
The proof follows by observing that from~\eqref{Eq_ProofThT2_setofmeasures},  it holds that
	%
	\begin{equation}
	\label{Eq_ProofT2supset}
		\bigcirc_{Q}(\set{M}) \subseteq \ \bigtriangledown_{Q}(\set{M}).
	\end{equation}
	%
	Hence, from~\eqref{Eq_ProofT2supset}, it follows that
	\begin{equation}
	\min_{P \in \bigtriangledown_{Q}}
	\foo{R}_{\dset{z}}(P) + \lambda \KL{Q}{P} \leq \min_{P \in \bigcirc_{Q}} \foo{R}_{\dset{z}}(P) + \lambda \KL{Q}{P}.
	\label{Eq_ProofT2IneOp}
	\end{equation}
	%
From the inequalities in \eqref{Eq_ProofT2IneOp2} and \eqref{Eq_ProofT2IneOp}, it follows that
	\begin{equation}
	\min_{P \in \bigtriangledown_{Q}}
	\foo{R}_{\dset{z}}(P) + \lambda \KL{Q}{P} = \min_{P \in \bigcirc_{Q}} \foo{R}_{\dset{z}}(P) + \lambda \KL{Q}{P}.
	\label{Eq_ProofT2IneOpSophiaMakesItBig}
	\end{equation}
	Thus, the measure $\Pgibbs{\bar{P}}{Q}$ in~\eqref{EqGenpdfType2} is the solution of the optimization problem in~\eqref{EqOpType2ERMRERNormal}, which completes the proof of Theorem~\ref{Theo_ERMType2RadNikMutualAbs}. 
\end{IEEEproof}
%-------------------------------------------------%

Lemma~\ref{lemm_RadNikDevMutualIneq} implies that the solution to the optimization problem in~\eqref{EqOpType2ERMRERNormal} is in the set~$\bigcirc_{Q}(\set{M})$ in~\eqref{EqGenpdfType2}.
%
A consequence of this observation is the following corollary.

%-------------------------------------------------%
%        Corollary 
%-------------------------------------------------%
\begin{corollary}
\label{coro_mutuallyAbsCont}
The probability measures~$Q$ and~$\Pgibbs{\bar{P}}{Q}$ in~\eqref{EqGenpdfType2} are mutually absolutely continuous.
\end{corollary}
%-------------------------------------------------%
%
Corollary~\ref{coro_mutuallyAbsCont} also follows from Theorem~\ref{Theo_ERMType2RadNikMutualAbs} by observing that the solution to the \mbox{Type-II} ERM-RER  problem in~\eqref{EqOpType2ERMRERNormal} is expressed in terms of its \RadonNikodym derivative with respect to $Q$, which implies the absolute continuity of $\Pgibbs{\bar{P}}{Q}$ with respect to $Q$. The absolute continuity of the measure $Q$ with respect to $\Pgibbs{\bar{P}}{Q}$ follows from the optimization domain of the \mbox{Type-II} ERM-RER  problem.
%
From this perspective, Corollary \ref{coro_mutuallyAbsCont} conveys the fact that there does not exist a dataset that can overcome the inductive bias induced by the reference measure $Q$. That is, sets of models outside the support of $Q$ exhibit zero probability measure with respect to the measure $\Pgibbs{\bar{P}}{Q}$.

This observation is important as, at first glance, the \mbox{Type-II} relative entropy regularization for the ERM problem in~\eqref{EqOpType2ERMRERNormal} does not restrict the solution to be absolutely continuous with respect to the reference measure $Q$.
% 
However, Theorem~\ref{Theo_ERMType2RadNikMutualAbs} shows that the support of the probability measure $\Pgibbs{\bar{P}}{Q}$ in~\eqref{EqGenpdfType2} collapses into the support of the reference.
%
A parallel can be established between \mbox{Type-I} and \mbox{Type-II} cases, as in both cases, the support of the solution is the support of the reference measure.
%
In a nutshell, the use of relative entropy regularization inadvertently forces the solution to coincide with the support of the reference regardless of the training data.


%-------------------------------------------------%
%
%      				Final Remarks
%
%-------------------------------------------------%
\section{Final Remarks}
\label{sec:FinalRemarks}

This work has introduced the \mbox{Type-II} ERM-RER problem and has presented its solution through Theorem~\ref{Theo_ERMType2RadNikMutualAbs}.
%%
The solution highlights that regardless of whether \mbox{Type-I} or \mbox{Type-II} regularization  is used in ERM problems, the models that are considered by the resulting solution are necessarily in the support of the reference measure. In this sense, the restriction over the models introduced by the reference measure cannot be bypassed by the training data when relative entropy is used as the regularizer.
%
This limitation has been shown to be a consequence of the equivalence that can be established between \mbox{Type-I} and \mbox{Type-II} regularization. 
%
These analytical results lead to providing an operationally meaningful characterization of the expected empirical risk induced by the \mbox{Type-II} solution in terms of the regularization parameters.  
%
  

\section{Acknowledgments}
This work is supported by the University of Sheffield ACSE PGR scholarships, the Inria Exploratory Action -- Information and Decision Making (AEx IDEM), the European Commission through the H2020-MSCA-RISE-2019 project TESTBED2 under grant agreement no. 872172, and in part by a grant from the C3.ai Digital Transformation Institute.



\appendices
%-------------------------------------------------%   
%   ___                  _____           
%  / _ \                /  ___|          
% / /_\ \_ __  _ ____  _\ `--.  ___  ___ 
% |  _  | '_ \| '_ \ \/ /`--. \/ _ \/ __|
% | | | | |_) | |_) >  </\__/ /  __/ (__ 
% \_| |_/ .__/| .__/_/\_\____/ \___|\___|
%       | |   | |                        
%       |_|   |_|                      
%-------------------------------------------------%
%<*appxsec>
\section{Proof of Lemma~\ref{lemm_Type2RNDbigcirc}}
%
%</appxsec>

%-------------------------------------------------%
% ______                 __ 
% | ___ \               / _|
% | |_/ / __ ___   ___ | |_ 
% |  __/ '__/ _ \ / _ \|  _|
% | |  | | | (_) | (_) | |  
% \_|  |_|  \___/ \___/|_| 
%-------------------------------------------------%
%<*proof>
\begin{IEEEproof}
\label{app_lemm_Type2RNDbigcirc}
%
The optimization problem in~\eqref{EqOpType2ERMRERancillary} can be re-written in terms of the Radon-Nikodym derivative of the optimization measure $P$ with respect to the measure $Q$, which yields:
		%
		\begin{subequations}
		\label{EqOpProblemType2fproof}
		\begin{IEEEeqnarray}{ccl}
		\min_{P \in \bigcirc_{Q}(\set{M})}
		&   & \ \int \foo{L}_{\vect{z}}(\thetav)\frac{\diff P}{\diff Q}(\thetav)\, \diff Q(\thetav) 
		%\nonumber\\ &   & 
		- \> \lambda \int \log(\frac{\diff P}{\diff Q})\diff Q(\thetav), \\
		\label{Eq_ProofT2Constraint}
		\mathrm{s.t.}  & &  \int \frac{\diff P}{\diff Q}(\thetav)  \diff Q(\thetav) = 1.
		\end{IEEEeqnarray}
		\end{subequations}
%
The remainder of the proof focuses on the problem in which the optimization is over the function  $\frac{\diff P}{\diff Q}: \set{M} \to \reals$, instead of optimizing the measure $P$. This is due to the fact that for all $P \in \bigcirc_{Q}\left( \set{M} \right)$, the Radon-Nikodym derivate $\frac{\diff P}{\diff Q}$ is unique up to sets of zero measure with respect to $Q$.
%
Let $\mathscr{M}$ be the set of measurable functions $\set{M} \to \reals$ with respect to the measurable spaces $\msblspc{\set{M}}$ and $\left( \reals, \BorSigma{\reals} \right)$ that are absolutely integrable with respect to $Q$.
That is, for all $\hat{g} \in \mathscr{M}$, it holds that
%
\begin{IEEEeqnarray}{rcl}
\label{EqValioLaPena}
\int \abs{ \hat{g} (\vect{\theta})} \diff Q(\vect{\theta}) & < & \infty .
\end{IEEEeqnarray}
 
%
Hence, the optimization problem of interest is:
%
\begin{subequations}
\label{EqOpProblemType2fproof9876}
\begin{IEEEeqnarray}{ccl}
\min_{g \in \mathscr{M}} & &  \int \foo{L}_{\vect{z}}(\thetav) g(\thetav)\, \diff Q(\thetav)- \lambda \int \log(g(\thetav))\diff Q(\thetav) \\
\label{Eq_ProofT2Constraint9876}
\mathrm{s.t.}  & &  \int g(\thetav)  \diff Q(\thetav) = 1.
\end{IEEEeqnarray}
\end{subequations}
%
Let the Lagrangian of the optimization problem in~\eqref{EqOpProblemType2fproof9876} be $L: \mathscr{M}\times \reals \rightarrow \reals$ such that
%
\begin{IEEEeqnarray}{rcl}
\label{EqFunctionalL}\label{EqFunctionalAll}
   L (g, \beta) 
   & = & \int \foo{L}_{\vect{z}}(\thetav)g(\thetav)\, \diff Q(\thetav) - \lambda \int \log(g(\thetav))\, \diff Q(\thetav)  
   %\nonumber\\&   & 
   + \> \beta (\int g(\thetav) \, \diff Q(\thetav)-1) \\
   & = & \int \Big( g(\thetav) (\foo{L}_{\vect{z}}(\thetav) + \beta)  - \lambda  \log(g(\thetav)) \Big)  \; \diff Q(\thetav)
   %\nonumber\\&   & 
   - \> \beta,
\end{IEEEeqnarray}
%
where $\beta$ is a  real that acts as a Lagrange multiplier due to the constraint~\eqref{Eq_ProofT2Constraint9876}.
%
Let $\hat{g}: \set{M} \rightarrow \reals$ be a function in $\mathscr{M}$. 
%
The Gateaux differential of the functional $L$ in~\eqref{EqFunctionalAll} at $\left(g, \beta\right) \in \mathscr{M}\times \reals$ in the direction of $\hat{g}$ is
%
\begin{IEEEeqnarray}{rcl}
\label{EqNecessaryCondtionType2f}
    \partial L(g, \beta; \hat{g} ) & \triangleq & \left.\frac{\diff}{\diff \gamma}  r(g + \gamma \hat{g}, \beta) \right|_{\gamma = 0},
\end{IEEEeqnarray}
%
where the function $r:\reals \rightarrow \reals$ is such that for all $\gamma \in (-\epsilon, \epsilon)$, with $\epsilon$ arbitrarily small, 
%The proof continues under the assumption that the function $g$ and $\hat{g}$ are such that the Gateaux differential in \eqref{EqNecessaryCondtionType2f} exists. Under such an assumption, let the function $r: \reals \rightarrow \reals$ satisfy for all $\gamma \in (-\epsilon, \epsilon)$, with $\epsilon$ arbitrarily small, that
%
\begin{subequations}
\label{EqrType2f}
\begin{IEEEeqnarray}{rcl}
    r(\gamma) %& = & L(g + \gamma \hat{g}, \beta) \\ 
    & = & \int \foo{L}_{\vect{z}}(\thetav)(g (\thetav) + \gamma \hat{g}(\thetav))\, \diff Q(\thetav)
    %\nonumber \\&   &
    - \> \lambda \int \log(g(\thetav) + \gamma \hat{g}(\thetav))\, \diff Q(\thetav) 
    %\nonumber \\ &    & 
    + \> \beta(\int(g(\thetav)  + \gamma \hat{g}(\thetav))\diff Q(\thetav) - 1) \\
	& = &  \gamma \int\hat{g}(\thetav)(\foo{L}_{\vect{z}}(\thetav)  +\beta)\diff Q(\thetav) 
	%\nonumber\\&   &
	+ \> \lambda \int \log(g(\thetav) + \gamma \hat{g}(\thetav))\, \diff Q(\thetav)
	%\nonumber\\ &    &
	+ \int g (\thetav) ( \foo{L}_{\vect{z}}(\thetav)  + \beta) \diff Q(\thetav) - \beta.\label{EqYoLloroCorrigiendoEstaPrueba}
\end{IEEEeqnarray}
\end{subequations}
%
Note that the first term in \eqref{EqYoLloroCorrigiendoEstaPrueba} is linear with respect to $\gamma$; the second term can be written using the function $\hat{r}:\reals \rightarrow \reals$ in \eqref{EqHatR_lemmAppx} such that
%
\begin{IEEEeqnarray}{rCl}
\label{EqHatR_lemmAppBDef}
	\hat{r}(\gamma) & = & \int -\log(g(\thetav) + \gamma \hat{g}(\thetav)) \diff Q(\thetav);
\end{IEEEeqnarray}
%
and the remaining terms are independent of $\gamma$.

Hence, based on the fact that the function $\hat{r}$ in \eqref{EqHatR_lemmAppBDef} is differentiable at zero (Lemma~\ref{lemm_ERM_fDR_DiffZero}), so is the function $r$ in \eqref{EqrType2f}, which implies that the Gateaux differential of $\partial L(g,\beta;\hat{g})$ in \eqref{EqNecessaryCondtionType2f} exists.

The derivative of the real function $r$ in \eqref{EqrType2f} is

\begin{IEEEeqnarray}{rcl}
  \!\!\frac{\diff}{\diff \gamma}r(\gamma)\,& = & \int\hat{g}(\thetav)(\foo{L}_{\vect{z}}(\thetav)  +\beta)\diff Q(\thetav)
  %\nonumber\\&   &
  - \lambda \int \frac{\hat{g}(\thetav)}{(g(\thetav) + \gamma \hat{g}(\thetav))} \, \diff Q(\thetav)\\
  \label{EqDerrType2f} & = & \int\hat{g}(\thetav)(\foo{L}_{\vect{z}}(\thetav)  +\beta \vphantom{\dot{f}} -  \frac{\lambda}{(g(\thetav) + \gamma \hat{g}(\thetav))} )\! \diff Q(\thetav).\IEEEeqnarraynumspace
\end{IEEEeqnarray}
From \eqref{EqNecessaryCondtionType2f} and \eqref{EqDerrType2f}, it follows that
%
\begin{IEEEeqnarray}{rcl}
\label{EqGateauxDiffType2f}
	\partial L(g, \beta; \hat{g} )
	& = & \int\hat{g}(\thetav)(\foo{L}_{\vect{z}}(\thetav)  +\beta \vphantom{\dot{f}} -  \frac{\lambda}{g(\thetav)} )\! \diff Q(\thetav). 
\end{IEEEeqnarray}
%
The relevance of the Gateaux differential in~\eqref{EqGateauxDiffType2f} stems from \cite[Theorem $1$, page $178$]{luenberger1997bookOptimization}, which unveils the fact that a necessary condition for the functional $L$ in~\eqref{EqFunctionalAll} to have a stationary point at $( \frac{\diff  \Pgibbs{P}{Q}}{\diff Q}, \beta ) \in \set{M} \times [0, \infty)$ is that for all functions $\hat{g} \in \mathscr{M}$,
%
\begin{equation}
\label{EqConditionhiType2f}
\partial L(\frac{\diff \Pgibbs{P}{Q}}{\diff Q}, \beta; \hat{g}) = 0 .
\end{equation}
%
From~\eqref{EqGateauxDiffType2f} and~\eqref{EqConditionhiType2f}, it follows that $\frac{\diff \Pgibbs{P}{Q}}{\diff Q} $ must satisfy for all functions $\hat{g}$ in $\mathscr{M}$ that
%
\begin{equation}
\label{EqIntegralHisZero}
 \int\hat{g}(\thetav)(\foo{L}_{\vect{z}}(\thetav)  + \beta  -  \lambda 
 ( \frac{\diff \Pgibbs{P}{Q}}{\diff Q}(\thetav) )^{-1}  ) \diff Q(\thetav) = 0.
\end{equation}
%
This implies that for all $\thetav \in \supp Q$,
%
\begin{equation}
\label{Eq_ProofT2SemiLemma_proof}
\foo{L}_{\vect{z}}(\thetav)  +\beta  -  \lambda  (\frac{\diff \Pgibbs{P}{Q}}{\diff Q}(\thetav)  )^{-1}   = 0,
\end{equation}
%
and thus,
%
\begin{equation}
\label{EqKeyNorthStarx}
  \frac{\diff \Pgibbs{P}{Q}}{\diff Q}(\thetav) = \frac{\lambda}{\beta + \foo{L}_{\dset{z}}(\thetav)},
\end{equation}
%
where $\beta$ is chosen to satisfy \eqref{Eq_ProofT2Constraint9876} and guarantee that for all $\vect{\theta} \in \supp Q$, it holds that $ \frac{\diff \Pgibbs{P}{Q}}{\diff Q}(\thetav) \in (0, \infty)$. That is, 
\begin{equation}
\beta \in \left\lbrace t\in \reals: \forall \vect{\theta} \in \supp Q , 0 < \frac{\lambda}{t + \foo{L}_{\vect{z}}(\thetav)}  \right\rbrace, \text{ and} 
\end{equation}
%
\begin{equation}
1 = \int \frac{\lambda}{\foo{L}_{\dset{z}}(\thetav)+\beta}\diff Q(\thetav).
\end{equation}
which is an assumption of the theorem.

The proof continues by verifying that the measure $\Pgibbs{\bar{P}}{Q}$ that satisfies \eqref{EqKeyNorthStarx} is the unique solution to the optimization problem in \eqref{EqOpProblemType2fproof}. Such verification is done by showing that the objective function in \eqref{EqOpProblemType2fproof} is strictly convex with the optimization variable. Let $P_1$ and $P_2$ be two different probability measures in $\msblspc{\set{M}}$ and let $\alpha$ be in $(0,1)$. Hence,
%
\begin{IEEEeqnarray}{rCl}
	%\IEEEeqnarraymulticol{3}{l}{
	\foo{R}_{\dset{z}} (\alpha P_1 +(1-\alpha)P_2) + \lambda\KL{\alpha P_1 +(1-\alpha)P_2}{Q}
	%}\nonumber \quad\\
	& = & \foo{R}_{\dset{z}}(\alpha P_1) + \foo{R}_{\dset{z}}((1-\alpha)P_2) 
	%\nonumber\\ &   & 
	+ \lambda\KL{\alpha P_1 +(1-\alpha)P_2}{Q}\\
	& > & \alpha\foo{R}_{\dset{z}}( P_1) + (1-\alpha)\foo{R}_{\dset{z}}(P_2) \nonumber\\
	&   & +\> \lambda(\alpha\KL{ P_1}{Q}+ (1-\alpha)\KL{P_2}{Q})
\end{IEEEeqnarray}

where the functional $\foo{R}_{\dset{z}}$ is defined in \eqref{EqRxy}. 
%
The equality above follows from the properties of the Lebesgue integral, while the inequality follows from \cite[Theorem 2]{perlaza2024ERMRER}.
%
This proves that the solution is unique due to the strict concavity of the objective function, which completes the proof.
%	 
\end{IEEEproof}
%</proof>
%-------------------------------------------------%

This appendix concludes by presenting Lemma~\ref{lemm_ERM_fDR_DiffZero} used in the proof of Lemma~\ref{lemm_Type2RNDbigcirc}
\begin{lemma}
\label{lemm_ERM_fDR_DiffZero}
Let $\field{M}$ be the set of measurable functions $h:\set{M} \rightarrow \reals$, with respect to the measurable space $\msblspc{\set{M}}$ and $\bormsblspc{\reals}$. Let $\field{S}$ be the subset of $\field{M}$ including all nonnegative functions that are absolutely integrable with respect to a probability measure $Q$. That is, for all $h \in \field{S}$, it holds that
%
\begin{IEEEeqnarray}{rCl}
\int \abs{h(\thetav)}\diff Q(\thetav) & < & \infty.
\end{IEEEeqnarray}
%
Let the function $\hat{r}: \reals \rightarrow \reals$ be such that
%
\begin{IEEEeqnarray}{rCl}
\label{EqHatR_lemmAppx}
	\hat{r}(\alpha) & = & \int -\log(g(\thetav) + \alpha h(\thetav)) \diff Q(\thetav),
\end{IEEEeqnarray}
%
for some functions $g$ and $h$ in $\field{S}$ and $\alpha \in (-\epsilon, \epsilon)$, with $\epsilon > 0$ arbitrarily small. The function $\hat{r}$ in \eqref{EqHatR_lemmAppx} is differentiable at zero.
\end{lemma}
\begin{IEEEproof}
The objective is to prove that the function $\hat{r}$ in \eqref{EqHatR_lemmAppx} is differentiable at zero, which boils down to proving that the limit
%
\begin{IEEEeqnarray}{rCl}
\label{EqLimf}
	& \lim_{\delta \rightarrow 0} \frac{1}{\delta}(\hat{r}(\alpha+\delta) -\hat{r}(\alpha))&
\end{IEEEeqnarray}
%
exists for all $\alpha \in (-\epsilon, \epsilon)$, with $\epsilon > 0$ arbitrarily small. 
%
Let the function $f:(0,\infty)\rightarrow \reals$ be a function such that
%
\begin{IEEEeqnarray}{rCl}
	f(x) & = & -\log(x).
\end{IEEEeqnarray}
%
Note that the function $\hat{r}$ can be written in terms of $f$ as follows
%
\begin{IEEEeqnarray}{rCl}
\label{EqHatR_lemmAppx_fRevKL}
	\hat{r}(\alpha) & = & \int f(g(\thetav) + \alpha h(\thetav)) \diff Q(\thetav),
\end{IEEEeqnarray}
%
The proof of the existence of such limit in \eqref{EqLimf} relies on the fact that the function $f$ in \eqref{EqHatR_lemmAppx} is strictly convex and differentiable, which implies that $f$ is also Lipschitz continuous. Hence, it follows that
%
\begin{IEEEeqnarray}{rCl}
\label{EqfisLipschitz}
	\abs{f(g(\thetav) + (\alpha+\delta) h(\thetav))-f(g(\thetav) + \alpha h(\thetav))} & \leq & c\abs{h(\thetav)}\abs{\delta},
\end{IEEEeqnarray}
%
for some positive and finite constant $c$, which implies that
%
\begin{IEEEeqnarray}{rCl}
\label{EqDfDxLipschitz}
	\frac{\abs{f(g(\thetav) + (\alpha+\delta) h(\thetav))-f(g(\thetav) + \alpha h(\thetav))}}{\abs{\delta}} & \leq & c\abs{h(\thetav)},
\end{IEEEeqnarray}
%
and thus, given that $h \in \field{S}$, it holds that
%
\begin{IEEEeqnarray}{rCl}
\label{EqLimitExists}
	\int \frac{\abs{f(g(\thetav) + (\alpha+\delta) h(\thetav))-f(g(\thetav) + \alpha h(\thetav))}}{\abs{\delta}} \diff Q(\thetav) & \leq & \infty.
\end{IEEEeqnarray}
%
This allows using the dominated convergence theorem as follows. From the fact that the function $f$ is differentiable, let $\dot{f}:\reals \rightarrow \reals$ be the first derivative of $f$. The limit in \eqref{EqLimf} satisfies for $\alpha \in (-\epsilon,\epsilon)$, with $\epsilon > 0$ arbitrarily small,
%
\begin{subequations}
\label{EqLimHatrDCT}
	\begin{IEEEeqnarray}{rCl}
	\lim_{\delta \rightarrow 0} \frac{1}{\delta}(\hat{r}(\alpha+\delta) -\hat{r}(\alpha)) 
	& = & \lim_{\delta \rightarrow 0} \frac{1}{\delta}(\int f(g(\thetav) + (\alpha+\delta) h(\thetav)) \diff Q(\thetav)-\int f(g(\thetav) + \alpha h(\thetav)) \diff Q(\thetav))
	\label{EqLimHatrDCT_s1}\\
	& = & \lim_{\delta \rightarrow 0} \int \frac{1}{\delta}(f(g(\thetav) + (\alpha+\delta) h(\thetav)) - f(g(\thetav) + \alpha h(\thetav))) \diff Q(\thetav)
	\label{EqLimHatrDCT_s2}\\
	& = & \int \lim_{\delta \rightarrow 0} \frac{1}{\delta}(f(g(\thetav) + (\alpha+\delta) h(\thetav)) - f(g(\thetav) + \alpha h(\thetav))) \diff Q(\thetav)
	\label{EqLimHatrDCT_s3}\\
	& = & \int \dot{f}(g(\thetav) + (\alpha+\delta) h(\thetav))  \diff Q(\thetav)
	\label{EqLimHatrDCT_s4}\\
	& < &  \infty,
	\label{EqLimHatrDCT_s5}
	\end{IEEEeqnarray}
\end{subequations}
%
where the equalities in \eqref{EqLimHatrDCT_s3} and \eqref{EqLimHatrDCT_s5} follow from the dominated convergence theorem \cite[Theorem 1.6.9]{ash2000probability}. From \eqref{EqLimHatrDCT_s5}, it follows that the function $\hat{r}$ in \eqref{EqHatR_lemmAppx} is differentiable at zero. This completes the proof.
\end{IEEEproof}


%\IEEEtriggeratref{24}
\bibliographystyle{IEEEtranlink}%IEEEtran
%\bibliographystyle{../../WeeklyReport/Bibliography/MyIEEEtran}
\bibliography{iEEEtranBibStyle.bib}


%\section{Biography Section}
%%
%\begin{IEEEbiographynophoto}{Francisco Daunas} received the M.Sc. degree in Advanced Control and Systems Engineering from The University of Sheffield, United Kingdom, in 2019. He is currently a Ph.D. candidate at the Department of Automatic Control and Systems Engineering of The University of Sheffield, United Kingdom. His research interests span information theory, statistical learning, and machine learning.
%\end{IEEEbiographynophoto}
%\balance
%
%\begin{IEEEbiographynophoto}{IÃ±aki Esnaola} (Member, IEEE) received the M.Sc. degree in Electrical Engineering from University of Navarra, Spain in 2006 and a Ph.D. in Electrical Engineering from University of Delaware, Newark, DE in 2011. He is currently a Senior Lecturer in the Department of Automatic Control and Systems Engineering of The University of Sheffield, and a Visiting Research Collaborator in the Department of Electrical Engineering of Princeton University, Princeton, NJ. In 2010-2011 he was a Research Intern with Bell Laboratories, Alcatel-Lucent, Holmdel, NJ, and in 2011-2013, he was a Postdoctoral Research Associate at Princeton University, Princeton, NJ. His research interests include information theory and communication theory with an emphasis on the application to smart grid problems.
%\end{IEEEbiographynophoto}
%
%
%\begin{IEEEbiographynophoto}{Samir M. Perlaza} (Senior Member, IEEE) is a research scientist with the Institut National de Recherche en Informatique et en Automatique (INRIA), France, and a visiting research scholar at the Department of Electrical Engineering of Princeton University, Princeton, (NJ, USA). He received the M.Sc. and Ph.D. degrees from \'Ecole Nationale Sup\'erieure des T\'el\'ecommunications (Telecom ParisTech), Paris, France, in 2008 and 2011, respectively. Previously, from 2008 to 2011, he was a Research Engineer at France T\'el\'ecom - Orange Labs (Paris, France). He has held long-term academic appointments at the Alcatel-Lucent Chair in Flexible Radio at SupÃ©lec (Gif-sur-Yvette, France); at Princeton University (Princeton, NJ) and at the University of Houston (Houston, TX). Dr. Perlaza currently serves as an Editor of the IEEE Transactions on Communications. Dr. Perlaza has been distinguished by the European Commission with an Alban Fellowship in 2006 and a Marie SkÅodowska-Curie Fellowship in 2015. His research interests lie in the overlap of signal processing, information theory, game theory and wireless communications.
%\end{IEEEbiographynophoto}
%
%\begin{IEEEbiographynophoto}{H. Vincent Poor}(Life Fellow, IEEE) received the Ph.D. degree in electrical engineering and computer science from Princeton University in 1977. From 1977 until 1990, he was on the faculty of the University of Illinois at Urbana-Champaign. Since 1990, he has been on the faculty at Princeton, where he is currently the Michael Henry Strater University Professor of Electrical Engineering. During 2006 to 2016, he served as the dean of Princetonâs School of Engineering and Applied Science. He has also held visiting appointments at several other universities, including most recently at Berkeley and Cambridge. His research interests include the areas of information theory, machine learning and network science, and their applications in wireless networks, energy systems and related fields. Among his publications in these areas is the forthcoming book Machine Learning and Wireless Communications (Cambridge University Press, 2021). He is a member of the National Academy of Engineering and the National Academy of Sciences, and is a foreign member of the Chinese Academy of Sciences, the Royal Society, and other national and international academies. Recent recognition of his work includes the 2017 IEEE Alexander Graham Bell Medal and a DEng honoris causa from the University of Waterloo, awarded in 2019.
%\end{IEEEbiographynophoto}



\end{document}


